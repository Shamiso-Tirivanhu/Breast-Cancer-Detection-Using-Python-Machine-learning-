#  Breast-Cancer-Detection using Python & Machine Learning-project

Table of Content
1. [Overview](#overview)
2. [Features](#features)
3. [Technological Uses](#technologuical-uses)
4. [Results](#ewsults)
5. [Conclusion](#conclusion)

## 1. Overview

This project focuses on leveraging machine learning techniques to classify breast cancer cases as malignant or benign based on medical data as shown in the image below. The goal is to provide a reliable and efficient way to assist in early diagnosis using Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Trees.


![Image_Alt](https://github.com/Shamiso-Tirivanhu/Breast-Cancer-Detection-project/blob/f89a56ab7cb87f7f54ca437e666aaf0cd4b05a91/A%20diagram%20of%20benign%20and%20malignant%20cells.png)


The image above illustrates a couple of people have malignant aka cancerous cells compared to those with benign aka non-cancerous cells. It does not necessarily mena that these indiviuals will die of cancer, it simply depicts that they hace it.




## 2. Features

- Data preprocessing & visualization
- Feature scaling & correlation analysis
- Multiple ML models for classification
- Performance evaluation using accuracy, confusion matrix & classification report

## 3. Technological Used

- Python
- Pandas & Numpy for Data Manipulation
- Matplotlib & Seaborn for Data visualization
- Sckit-Learn for Machine Learning algirthms 

## 3. Usage

1. Koad the dataset Breast_cancer_csv
2. Preprocessing and Data Visualization
3. Train models and evaluate their performance

## 4. Results

| Model | Accuracy | Key Insights
--------|----------|--------------|
| Logistic Regression | 95% | Relaible & interpretable minimal false positives |
| KNN | 95% | Sensitive to neigbour selection, competitive results |
| SVM | 95% | Best performer, excellent class separation |
| Decision Tree | 88% | Prone to overfitting , lower accuracy |

## 5. Conclusion 

- Logistic Regression (96%): Performed well with high accuracy and minimal false positives, making it a reliable choice for interpretability.
- K-Nearest Neighbors (95%): Showed competitive performance but is sensitive to the choice of neighbors, which may affect predictions.
- Support Vector Machine (97%): Outperformed other models by efficiently separating classes, making it the best performer in this study.
- Decision Tree (88%): Provided interpretability but had lower accuracy, likely due to overfitting on the training data.

- Among all models, SVM proved to be the most effective, achieving the highest accuracy at 97%. However, further improvements can be explored using ensemble methods or deep learning techniques for even better performance.

Check out the code and feel free to contribute 


